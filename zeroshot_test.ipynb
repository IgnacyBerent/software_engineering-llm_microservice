{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain_core.runnables import RunnableSequence\n",
    "from langchain_core.output_parsers.json import SimpleJsonOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_models_list = [\n",
    "    \"llama3.2:latest\",\n",
    "    \"mistral:latest\",\n",
    "    \"mistrallite:latest\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "template1 = \"\"\"\n",
    "Summarize shortly the most important aspects of the plot of the following conversation between multiple users:\n",
    "\n",
    "{conversation}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "template2 = \"\"\"\n",
    "Give general idea about the topic of the following conversation between multiple users:\n",
    "\n",
    "{conversation}\n",
    "\n",
    "Topic:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation1 = \"\"\"\n",
    "User1: Hi, how are you?\n",
    "User2: I'm good, thanks! How about you?\n",
    "User1: I'm doing well. Did you finish the project?\n",
    "User2: Yes, I submitted it yesterday.\n",
    "User1: Great! Let's discuss the next steps.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "conversation2 = \"\"\"\n",
    "User1: Python has the worst package manager I've worked with, IMO.\n",
    "User2: Npm?\n",
    "User3: What's so bad about npm?\n",
    "User2: I used npx CRA yesterday, and three errors popped up on a stable version.\n",
    "User1: I also have no clue what problem Anaconda is supposed to solve, because from my experience, it's even more complicated than venv.\n",
    "User3: Well, you're answering your own question.\n",
    "User3: Just don't use CRA.\n",
    "User3: Vite.\n",
    "User3: npm create vite@latest\n",
    "User4: Agreed, Vite is better.\n",
    "User3: Even the official React documentation now prefers Vite over CRA.\n",
    "User2: Oh, cool.\n",
    "User2: Thanks.\n",
    "User5: C++ is watching.\n",
    "User3: Alright, you win.\n",
    "User1: I've never written in C++.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: llama3.2:latest\n",
      "The conversation between User1 and User2 is about a completed project. User2 has finished submitting the project, and User1 expresses enthusiasm for discussing the next steps.\n",
      "-------------------------------------------\n",
      "Model: mistral:latest\n",
      " The conversation revolves around two users discussing a completed project and planning their next steps together. User1 inquires about the status of the project, to which User2 confirms that it has been submitted. They then decide to move forward by discussing the next steps they will take.\n",
      "-------------------------------------------\n",
      "Model: mistrallite:latest\n",
      "The conversation appears to be between two individuals discussing a project they had been working on. User2 has completed the project and submitted it for review, while User1 is interested in discussing the next steps.\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for model in ollama_models_list:\n",
    "    llm = OllamaLLM(model=model)\n",
    "    prompt = PromptTemplate.from_template(template1)\n",
    "    chain = prompt | llm \n",
    "    print(f\"Model: {model}\")\n",
    "    output = chain.invoke(input={'conversation':conversation1})\n",
    "    print(output)\n",
    "    print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: llama3.2:latest\n",
      "The topic of this conversation appears to be a project that User1 and User2 are working on together. Specifically, they seem to be discussing the completion of the project and planning for what comes next, such as submitting it or moving forward with further work. The conversation is likely related to a professional or academic setting, possibly in the fields of business, education, or research.\n",
      "-------------------------------------------\n",
      "Model: mistral:latest\n",
      " The topic of this conversation appears to be about a project that User2 has completed and is now moving forward with the next stages or tasks related to it. User1 expresses interest in discussing these next steps with User2, indicating potential collaboration or continuation of the project. It's not specified what kind of project they are referring to; it could be anything from a work-related assignment to a personal creative endeavor.\n",
      "-------------------------------------------\n",
      "Model: mistrallite:latest\n",
      "\n",
      "Hi there, I'm glad to see that both of you are doing well and have completed your respective projects. Now that the project is finished, let's discuss the next steps. First, it would be a great idea to review the project and see if there are any areas that can be improved or optimized. Are there any new features or functionalities that could be added?\n",
      "\n",
      "Secondly, it's important to plan for future development and set clear goals and deadlines. This will help ensure that everyone is on the same page and working towards a common goal. Is there anything specific that you would like to achieve in the next project? Are there any new technologies or tools that could be used?\n",
      "\n",
      "Thirdly, it's always good to celebrate successes and recognize accomplishments. Take some time to congratulate each other for completing the project and working well together as a team.\n",
      "\n",
      "Last but not least, keep open lines of communication and continue to support one another. If there are any issues or challenges, be sure to communicate them early on so that they can be addressed promptly.\n",
      "\n",
      "Does this cover everything? Let me know if you have any further questions or concerns.\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for model in ollama_models_list:\n",
    "    llm = OllamaLLM(model=model)\n",
    "    prompt = PromptTemplate.from_template(template2)\n",
    "    chain = prompt | llm \n",
    "    print(f\"Model: {model}\")\n",
    "    output = chain.invoke(input={'conversation':conversation1})\n",
    "    print(output)\n",
    "    print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: llama3.2:latest\n",
      "The conversation is about package managers and build tools for JavaScript projects. The main points are:\n",
      "\n",
      "* User1 expresses frustration with Python's package manager\n",
      "* User2 mentions npm as a comparison\n",
      "* User3 defends npm, but User2 shares an experience where npx (CRA) caused errors on a stable version\n",
      "* Vite is recommended as a better alternative to CRA and is even preferred by the official React documentation\n",
      "* C++ makes a brief appearance at the end, with no relevance to the main discussion.\n",
      "-------------------------------------------\n",
      "Model: mistral:latest\n",
      " The conversation revolves around the criticism of package managers (Python, npm) and environment management tools (Anaconda, venv), with specific examples of issues encountered (errors with npx CRA and complexity with Anaconda). A consensus emerges that Vite is a better alternative for creating React projects, which is also suggested by the official React documentation. Additionally, there's a humorous mention that C++ is observing the discussion.\n",
      "-------------------------------------------\n",
      "Model: mistrallite:latest\n",
      "\n",
      "In this conversation, the participants discussed various package managers and build tools for different programming languages. User1 expressed dissatisfaction with Python's package manager, while User2 encountered issues with npm and CRA. User3 suggested using Vite instead of CRA and provided a command to create a new project using Vite. The conversation then shifted to the complexity of Anaconda compared to venv, and User4 agreed that Vite was a better alternative to CRA. Finally, User5 made a light-hearted joke about C++ watching the conversation.\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for model in ollama_models_list:\n",
    "    llm = OllamaLLM(model=model)\n",
    "    prompt = PromptTemplate.from_template(template1)\n",
    "    chain = prompt | llm \n",
    "    print(f\"Model: {model}\")\n",
    "    output = chain.invoke(input={'conversation':conversation2})\n",
    "    print(output)\n",
    "    print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: llama3.2:latest\n",
      "The conversation appears to be about package managers and build tools for front-end development, specifically comparing npm (Node Package Manager) with CRA (Create React App), a popular tool for setting up React projects. The users are discussing their personal experiences with these tools and sharing recommendations, such as Vite as an alternative.\n",
      "-------------------------------------------\n",
      "Model: mistral:latest\n",
      " Discussion about the pros and cons of various package managers (Python, npm, Anaconda) for different projects, with a focus on specific issues encountered during project development. The conversation also features alternative solutions such as Vite (a modern front-end build tool), and an indirect comparison to the programming language C++.\n",
      "-------------------------------------------\n",
      "Model: mistrallite:latest\n",
      "\n",
      "User1 stated that Python has the worst package manager they have worked with, and User2 asked about Npm. User3 responded that they used Npm's package, npx CRA, which resulted in three errors appearing on a stable version. User1 then expressed their confusion about Anaconda and its purpose, stating it is even more complicated than venv.\n",
      "\n",
      "User3 suggested that the user simply not use CRA, recommending Vite instead. User2 was appreciative of this suggestion and agreed that Vite is better.\n",
      "\n",
      "Furthermore, the official React documentation has now started to recommend using Vite over CRA, which User3 pointed out.\n",
      "\n",
      "Overall, this conversation seems to center around a disagreement over package managers and their respective benefits and drawbacks.\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for model in ollama_models_list:\n",
    "    llm = OllamaLLM(model=model)\n",
    "    prompt = PromptTemplate.from_template(template2)\n",
    "    chain = prompt | llm \n",
    "    print(f\"Model: {model}\")\n",
    "    output = chain.invoke(input={'conversation':conversation2})\n",
    "    print(output)\n",
    "    print(\"-------------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
